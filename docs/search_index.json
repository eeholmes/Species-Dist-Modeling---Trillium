[
["index.html", "SDMs - Trillium Example Overview 0.1 Prequisite knowledge 0.2 Set-up - R and RStudio 0.3 Get the shapefiles for Hubbard Brook 0.4 Set-up - R packages 0.5 Data downloads", " SDMs - Trillium Example E. E. Holmes 2020-10-02 Overview A project to learn how to do species distribution modeling (SDM) in R. Ultimately, I’ll probably use the biomod2 or Wallace packages but in the learning phase, I am building and running models manually. 0.1 Prequisite knowledge Before going through the code, you should have a basic understanding of spatial data and working with that data in R. The spatial manipulations done here are super simple but if you have no or very little exposure to spatial data terminology or the raster package in R, then go through this chapter on spatial data in R first. Secondly, you’ll need a basic understanding of generalized linear models (GLMs) and generalized additive models (GAMs). Go through this introduction to GLMs and go through this introduction to GAMs or this introduction on GAMs. For the purpose of the material here, a skim of these introductions is fine just so you have a basic understanding of what models are being used. 0.2 Set-up - R and RStudio If you have not updated R recently (in the last 6 months), go ahead and do that. Also update RStudio is you haven’t done that recently. Download R here Download RStudio here 0.3 Get the shapefiles for Hubbard Brook Create a project in RStudio for the SDM building. Within that project, create a folder called data and one called code. Within data create a folder called hbef_boundary. Go to the Species-Dist-Modeling—Trillium repository hbef_boundary folder and download all the files there into your hbef_boundary folder. 0.4 Set-up - R packages The code will use the following R packages which you will need to install. Open RStudio and go to the Packages tab on the right. Then click Install and search for the package. library(biomod2) library(dismo) library(sp) library(raster) library(ggplot2) library(maps) library(usdm) library(ecospat) library(corrplot) library(MASS) library(gam) library(stringr) # for easy string manipulation library(tidyr) # for data wrangling for ggplot library(knitr) # for R Markdown library(here) # for intelligent file directory navigation 0.5 Data downloads When you go through the Rmd files, it will download a lot of data into your project, but the next time you run the files, the code will look for the downloaded files and not rerun the downloads. "],
["trillium.html", "Chapter 1 Trillium", " Chapter 1 Trillium Trillium genus are long-lived, woodland, perennial wildflower found throughout eastern North America. In this example, I have temperature, precipitation and basic land cover data. I will use that to try to model Trillium distribution. Trillium grandiflorum occurs on well-drained, rich, mesic soils in deciduous or mixed deciduous/coniferous forests. Trillium undulatum occurs in mesic, northern hardwoods, mixed conifer-hardwood forests, to pinewoods and high-elevation red spruce forests in very acidic humus-rich soils. Red Trillium Painted Trillium Hubbard Brook is an experimental forest in New Hampshire. It is in a watershed surrounded by a ridge with Hubbard Brook flowing west to east. There are few Trillium observations in the GBIF database in the experimental forest though Trillium occurs there. "],
["shape-files.html", "Chapter 2 Shape files 2.1 Create the boundary box 2.2 Get the states shapefile 2.3 Get the Hubbard Brook boundary 2.4 Plot the boundaries together 2.5 Save", " Chapter 2 Shape files The first step is to define the spatial extent of the area you will be working with and set up any shape files you need to plot boundaries or polygons or mask out areas. library(sp) library(raster) library(maps) 2.1 Create the boundary box First I need to define an raster::extent object for a box bounding NH and VT. This will be used to crop the data that we download. I can get a bounding box dynamically using drawExtent(). Click twice (upper corner/lower corner) on the map to select the region of interest. Don’t go too far outside the lines. maps::map(&quot;state&quot;, region = c(&quot;new hampshire&quot;, &quot;vermont&quot;, &quot;new york&quot;, &quot;massachusetts&quot;)) NHVT &lt;- raster::drawExtent() or I can use longitude/latitude values for the box and use extent(): NHVT &lt;- raster::extent(-73.61056, -70.60205, 42.48873, 45.37969) 2.2 Get the states shapefile I download the shapefile for the NH and VT state borders using getData() which gives polygons for countries. Level 1 will be the state boundaries (I assume). The shape file has all the states. Then I use subset() to get the two states that I want. path says where to save the downloaded file. usashp &lt;- raster::getData(&quot;GADM&quot;, country = &quot;USA&quot;, level = 1, path = &quot;data&quot;) nhvtshp &lt;- subset(usashp, NAME_1 %in% c(&quot;New Hampshire&quot;, &quot;Vermont&quot;)) nhshp &lt;- subset(usashp, NAME_1 %in% c(&quot;New Hampshire&quot;)) vtshp &lt;- subset(usashp, NAME_1 %in% c(&quot;Vermont&quot;)) Check the projection for this shapefile: crs(nhvtshp) CRS arguments: +proj=longlat +datum=WGS84 +no_defs I can plot the shapes. plot(nhvtshp, border = &quot;blue&quot;, axes = TRUE) 2.3 Get the Hubbard Brook boundary I downloaded this shapefile separately and read it in. This will get the boundary of the Hubbard Brook Experimental Forest from a shapefile. Although I write “shapefile” singular, it is actually two files, the shapefile and some metafiles. If you look in the hbef_boundary folder you’ll the metafiles. hbshp &lt;- raster::shapefile(&quot;data/hbef_boundary/hbef_boundary.shp&quot;) I check its projection and note that it is different from the NH+VT shapefile. crs(hbshp) CRS arguments: +proj=utm +zone=19 +datum=NAD83 +units=m +no_defs I transform the shapefile to get it on the same projection. newcrs &lt;- crs(nhvtshp) hbshp &lt;- sp::spTransform(hbshp, newcrs) Plot it. plot(hbshp) 2.4 Plot the boundaries together plot(nhvtshp, border = &quot;blue&quot;, axes = TRUE) plot(hbshp, add = TRUE) text(-71.8, 44, &quot;HBEF&quot;, pos = 4) 2.5 Save I save the shapefile data to a file so I can use it later without rerunning this code. save(nhvtshp, hbshp, nhshp, vtshp, NHVT, file = &quot;data/shapefiles.RData&quot;) "],
["observation-data.html", "Chapter 3 Observation data 3.1 Set-up 3.2 Download data 3.3 Check the coordinate projection 3.4 Make a sp object 3.5 Check for inaccurate location data 3.6 Plot 3.7 Save", " Chapter 3 Observation data 3.1 Set-up This example will use the following libraries: library(dismo) library(sp) library(here) Load the shapefiles created earlier. load(&quot;data/shapefiles.RData&quot;) 3.2 Download data I will download occurrence data for Trillium grandiflorum and Trillium undulatum in my NHVT bounding box from the Global Biodiversity Information Facility. nrecs seems to be ignored. geo means only points with longitude and latitude. removeZeros means get rid of NA in location. ext is the bounding box to use. First I set where I will save the file and check if it is already there. I do this because if I rerun this script, I don’t want to re-download. Note that GBIF data is updated weekly so using a time-stamp on your file might be good, but I am not doing that for this example. filePath &lt;- file.path(here::here(), &quot;data/trillium_presences.RData&quot;) Now I download if I haven’t downloaded already. The downloaded data has many columns that I don’t need. I will subset the following columns. select in the subset() call says what columns to use. if (!file.exists(filePath)) { # Download grandiflorum &lt;- dismo::gbif(&quot;Trillium&quot;, species = &quot;grandiflorum&quot;, nrecs = 300, geo = TRUE, removeZeros = TRUE, ext = NHVT) undulatum &lt;- dismo::gbif(&quot;Trillium&quot;, species = &quot;undulatum&quot;, nrecs = 300, geo = TRUE, removeZeros = TRUE, ext = NHVT) trillium.raw &lt;- rbind(grandiflorum, undulatum) # select columns colsWeNeed &lt;- c(&quot;species&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;locality&quot;, &quot;year&quot;, &quot;coordinateUncertaintyInMeters&quot;, &quot;occurrenceID&quot;, &quot;occurrenceRemarks&quot;, &quot;geodeticDatum&quot;) trillium.raw &lt;- subset(trillium.raw, select = colsWeNeed) save(trillium.raw, file = &quot;data/trillium_presences.RData&quot;) } Load in the presences data (saved from code above). load(&quot;data/trillium_presences.RData&quot;) 3.3 Check the coordinate projection Check the projection to make sure it makes sense and there is only one value. Check that it is the same projection as my other layers. unique(trillium.raw$geodeticDatum) # &#39;WGS84&#39; [1] &quot;WGS84&quot; 3.4 Make a sp object trillium.raw is just a data frame. I make it a sp object (specifically a SpatialPointsDataFrame) using sp::coordinates() to specify which columns are the longitude and latitude. trillium &lt;- trillium.raw sp::coordinates(trillium) &lt;- c(&quot;lon&quot;, &quot;lat&quot;) Check that it looks ok and there are no NAs. summary(trillium$lon) Min. 1st Qu. Median Mean 3rd Qu. Max. -73.58 -73.04 -72.64 -72.44 -71.89 -70.61 summary(trillium$lat) Min. 1st Qu. Median Mean 3rd Qu. Max. 42.49 43.67 44.21 44.04 44.48 45.37 3.5 Check for inaccurate location data The coordinateUncertaintyInMeters column give the uncertainty of the observation location. Some of the uncertainties are huge and I don’t want those. table(cut(trillium$coordinateUncertaintyInMeters, c(0, 200, 500, 1000, 2000, 5000))) (0,200] (200,500] (500,1e+03] (1e+03,2e+03] (2e+03,5e+03] 1230 81 39 48 60 I am going to keep only those locations with a location accuracy within 200m. good &lt;- which(trillium$coordinateUncertaintyInMeters &lt; 200) trillium &lt;- trillium[good, ] 3.6 Plot Now I can plot the occurrences points and add the NH and VT state boundaries. Trillium undulatum is much more common. Hubbard Brook is outlined in blue. plot(nhvtshp, border = &quot;blue&quot;, axes = TRUE) plot(subset(trillium, species == &quot;Trillium grandiflorum&quot;), pch = 3, cex = 1, add = TRUE) plot(subset(trillium, species == &quot;Trillium undulatum&quot;), pch = 4, cex = 1, col = &quot;red&quot;, add = TRUE) plot(hbshp, add = TRUE, border = &quot;blue&quot;) 3.7 Save save(trillium, trillium.raw, file = &quot;data/trillium_presences.RData&quot;) "],
["variables.html", "Chapter 4 Variables 4.1 Set-up 4.2 Climatic data 4.3 Topographical data 4.4 Land cover data 4.5 Stack all variables 4.6 Fix layer names 4.7 Fix temperature in GBIF 4.8 Save", " Chapter 4 Variables I will download climate, landcover and elevation data to use as predictors. 4.1 Set-up This chapter will use the following libraries. These must be loaded for the code in this chapter to work. I’ve used :: to clarify what functions are associated with what packages, but still the packages must be loaded to do any plotting. library(sp) library(raster) library(stringr) I need to load in the shapefile data from Chapter 1. load(&quot;data/shapefiles.RData&quot;) I have found that when working with large raster layers that RStudio can sometimes get very slow. In that case a call to the “garbage collection” gc() function to clear out temporary memory can help. If it doesn’t then going to the Tools tab and restarting R will help. The latter will get rid of any variables in your working directory so make sure anything you need is saved. 4.2 Climatic data I will use the getData function from the raster package to get climate data. I will retrieve global bioclimatic variables at 0.5’ (1km) resolution from WorldClim. This returns 19 bioclimatic variables. For the 0.5’ resolution, I need to give it a center longitude and latitude and then it returns data centered on that in a 30 degrees longitude and latitude box. This is 300+ Mb but it will check if the directory exists and won’t keep re downloading if you re-run the code. path tells it where to save the downloaded directory (which will be called wc0.5). bioclimVars &lt;- raster::getData(name = &quot;worldclim&quot;, res = 0.5, var = &quot;bio&quot;, lon = mean(NHVT[1:2]), lat = mean(NHVT[3:4]), path = &quot;data&quot;) bioclimVars is a raster stack. class(bioclimVars) A raster stack is collection of many raster layers with the same projection, spatial extent and resolution. I don’t know why _13 is appended to the bioclim names by getData(). raster::extent(bioclimVars) # lons are x and lats are y class : Extent xmin : -90 xmax : -60 ymin : 30 ymax : 60 names(bioclimVars) # look at the variable names [1] &quot;bio1_13&quot; &quot;bio2_13&quot; &quot;bio3_13&quot; &quot;bio4_13&quot; &quot;bio5_13&quot; &quot;bio6_13&quot; [7] &quot;bio7_13&quot; &quot;bio8_13&quot; &quot;bio9_13&quot; &quot;bio10_13&quot; &quot;bio11_13&quot; &quot;bio12_13&quot; [13] &quot;bio13_13&quot; &quot;bio14_13&quot; &quot;bio15_13&quot; &quot;bio16_13&quot; &quot;bio17_13&quot; &quot;bio18_13&quot; [19] &quot;bio19_13&quot; 4.2.1 Crop and subset I will crop down this raster stack to the NH+VT bounding box. NHVTVars &lt;- raster::crop(bioclimVars, NHVT) Here I will plot just 3 of these variables. BIO10 = Mean Temperature of Warmest Quarter BIO15 = Precipitation Seasonality (how much precipitation varies during year) BIO19 = Precipitation of Coldest Quarter sub.NHVTVars &lt;- subset(NHVTVars, c(&quot;bio10_13&quot;, &quot;bio15_13&quot;, &quot;bio19_13&quot;)) Plot these three variables. This takes awhile. Note bio10_13 is mean temperature times 10. That’s how GBIF records temperature. plot(sub.NHVTVars) Another why to make this stack is to read in the data from the downloaded files in dir(\"wc0.5\"). Downloaded in .bil format and comprised of two files. fils &lt;- dir(&quot;data/wc0.5&quot;, full.names = TRUE) fils &lt;- fils[stringr::str_detect(fils, &quot;bil&quot;)] Plot one layer. onelayer &lt;- raster::raster(fils[1]) plot(onelayer, xlim = NHVT[1:2], ylim = NHVT[3:4]) # Add the NH VT lines on top plot(nhvtshp, add = TRUE, border = &quot;blue&quot;) plot(hbshp, add = TRUE) title(names(onelayer)) You can create a stack from the file names and then crop to the NH+VT bounding box. # But for some reason when it reads in the bil file, it is # not setting the projection from the header file. So I&#39;ll # use the NHVTVars &lt;- raster::stack(fils) NHVTVars &lt;- raster::crop(NHVTVars, NHVT) 4.2.2 Bioclim names I want to make a data frame to use to get the descriptions for the variables. I made this by looking at the WorldClim website. name is the name of the variable in the downloaded data, desc is a description, col is what I will call the column. bioclimnames &lt;- data.frame(name = paste0(&quot;bio&quot;, 1:19, &quot;_13&quot;), desc = c(&quot;Mean annual temperature&quot;, &quot;Mean diurnal range (mean of max temp - min temp)&quot;, &quot;Isothermality (bio2/bio7) (* 100)&quot;, &quot;Temperature seasonality (standard deviation *100)&quot;, &quot;Max temperature of warmest month&quot;, &quot;Min temperature of coldest month&quot;, &quot;Temperature annual range (bio5-bio6)&quot;, &quot;Mean temperature of the wettest quarter&quot;, &quot;Mean temperature of driest quarter&quot;, &quot;Mean temperature of warmest quarter&quot;, &quot;Mean temperature of coldest quarter&quot;, &quot;Total (annual) precipitation&quot;, &quot;Precipitation of wettest month&quot;, &quot;Precipitation of driest month&quot;, &quot;Precipitation seasonality (coefficient of variation)&quot;, &quot;Precipitation of wettest quarter&quot;, &quot;Precipitation of driest quarter&quot;, &quot;Precipitation of warmest quarter&quot;, &quot;Precipitation of coldest quarter&quot;), col = c(&quot;mean.temp&quot;, &quot;temp.diurnal.range&quot;, &quot;isotherm&quot;, &quot;temp.seasonality&quot;, &quot;max.warm.temp&quot;, &quot;min.cold.temp&quot;, &quot;temp.annual.range&quot;, &quot;mean.temp.wet.qtr&quot;, &quot;mean.temp.dry.qtr&quot;, &quot;mean.temp.warm.qtr&quot;, &quot;mean.temp.cold.qtr&quot;, &quot;total.precip&quot;, &quot;precip.wet.month&quot;, &quot;precip.dry.month&quot;, &quot;precip.seasonality&quot;, &quot;precip.wet.qtr&quot;, &quot;precip.dry.qtr&quot;, &quot;precip.warm.qtr&quot;, &quot;precip.cold.qtr&quot;), stringsAsFactors = FALSE) Do some memory clean-up before moving on. gc() used (Mb) gc trigger (Mb) limit (Mb) max used (Mb) Ncells 4609770 246.2 7515327 401.4 NA 7377009 394.0 Vcells 11515875 87.9 20629907 157.4 16384 20625175 157.4 4.3 Topographical data I can read in elevation data with getData(). For USA, this returns a list of 4 raster layers. List 1 is mainland. Then I crop to my NHVT bounding box. I don’t need to re-download the data if I already have it. path is the folder where the downloaded data will be stored. mask=FALSE means don’t cut off the elevation data at the US-Canada border. The USAelevation object is about 80MB. That is not needed so you might not want to save that file. dirPath &lt;- &quot;data/elevation&quot; if (!dir.exists(dirPath)) dir.create(dirPath) if (!file.exists(&quot;data/NHVTelevation.grd&quot;)) { USAelevation &lt;- raster::getData(&quot;alt&quot;, country = &quot;USA&quot;, path = &quot;data/elevation&quot;, mask = FALSE) raster::writeRaster(USAelevation[[1]], filename = &quot;data/USAelevation.grd&quot;, overwrite = TRUE) NHVT.elevation &lt;- raster::crop(USAelevation[[1]], NHVT) raster::writeRaster(NHVT.elevation, filename = &quot;data/NHVTelevation.grd&quot;, overwrite = TRUE) } else { NHVT.elevation &lt;- raster::raster(&quot;data/NHVTelevation.grd&quot;) } Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum Unknown based on WGS84 ellipsoid in CRS definition The plot of raw elevation is not so pretty. Later we will use the hillshade() function to make a nicer plot of elevation. plot(NHVT.elevation, axes = TRUE, legend.args = list(text = &quot;elevation (m)&quot;, adj = 0.2)) 4.3.1 Slope and aspect The terrain() function will return the slope and aspect from an elevation layer. NHVT.slope &lt;- raster::terrain(NHVT.elevation, opt = &quot;slope&quot;) NHVT.aspect &lt;- raster::terrain(NHVT.elevation, opt = &quot;aspect&quot;) Plot of aspect. plot(NHVT.aspect) plot(nhvtshp, add = TRUE) There is also a hillShade() function that makes a prettier elevation plot than the elevation data alone. hill &lt;- raster::hillShade(NHVT.slope, NHVT.aspect, 40, 270) plot(hill, col = grey(0:100/100), legend = FALSE, main = &quot;NH and VT elevation&quot;) plot(NHVT.elevation, col = rainbow(25, alpha = 0.35), add = TRUE) plot(nhvtshp, add = TRUE) plot(hbshp, add = TRUE) 4.4 Land cover data These data are downloaded from EarthEnv land cover data set with a function called getlandcover.R in the code folder. fils &lt;- paste0(&quot;data/landcover/lc_1km_&quot;, c(1:12, &quot;dom&quot;), &quot;.tif&quot;) # check if the files already exist, and if not, download if (!all(file.exists(fils))) { getlandcover(gisdir = &quot;data/landcover&quot;, ext = NHVT) } NHVT.landcover &lt;- raster::stack(fils) The names are cryptic. Fix that. oldnames &lt;- paste0(&quot;lc_1km_&quot;, c(1:12, &quot;dom&quot;)) newnames &lt;- c(&quot;Mixed.Needleleaf.Trees&quot;, &quot;Evergreen.Broadleaf.Trees&quot;, &quot;Deciduous.Broadleaf.Trees&quot;, &quot;Mixed.Other.Trees&quot;, &quot;Shrubs&quot;, &quot;Herbaceous&quot;, &quot;Cultivated&quot;, &quot;Flooded&quot;, &quot;Urban&quot;, &quot;Snow&quot;, &quot;Barren&quot;, &quot;Water&quot;, &quot;Dominant.Land.Cover&quot;) names(NHVT.landcover)[match(names(NHVT.landcover), oldnames)] &lt;- newnames I want to make a layer for trees which are in layers 1 to 4. for (i in 1:4) { fil &lt;- paste0(&quot;data/landcover/lc_1km_&quot;, i, &quot;.tif&quot;) r &lt;- raster(fil) if (i == 1) rt &lt;- r else rt &lt;- r + rt } NHVT.Trees &lt;- rt names(NHVT.Trees) &lt;- &quot;Tree.Cover&quot; Trillium undulatum is associated with tree cover. plot(NHVT.Trees) plot(nhvtshp, add = TRUE, border = &quot;blue&quot;) plot(hbshp, add = TRUE) title(&quot;Tree Cover&quot;) plot(subset(trillium, species == &quot;Trillium undulatum&quot;), pch = &quot;.&quot;, cex = 2, col = &quot;red&quot;, add = TRUE) plot(hbshp, add = TRUE, border = &quot;blue&quot;) 4.5 Stack all variables First, I will create a stack with all my variables. allVars &lt;- raster::stack(NHVTVars, NHVT.elevation, NHVT.slope, NHVT.aspect, NHVT.landcover, NHVT.Trees) 4.6 Fix layer names The names in allVars are annoying, so I’ll give it better names. names(allVars) [1] &quot;bio1_13&quot; &quot;bio2_13&quot; [3] &quot;bio3_13&quot; &quot;bio4_13&quot; [5] &quot;bio5_13&quot; &quot;bio6_13&quot; [7] &quot;bio7_13&quot; &quot;bio8_13&quot; [9] &quot;bio9_13&quot; &quot;bio10_13&quot; [11] &quot;bio11_13&quot; &quot;bio12_13&quot; [13] &quot;bio13_13&quot; &quot;bio14_13&quot; [15] &quot;bio15_13&quot; &quot;bio16_13&quot; [17] &quot;bio17_13&quot; &quot;bio18_13&quot; [19] &quot;bio19_13&quot; &quot;USA1_alt&quot; [21] &quot;slope&quot; &quot;aspect&quot; [23] &quot;Mixed.Needleleaf.Trees&quot; &quot;Evergreen.Broadleaf.Trees&quot; [25] &quot;Deciduous.Broadleaf.Trees&quot; &quot;Mixed.Other.Trees&quot; [27] &quot;Shrubs&quot; &quot;Herbaceous&quot; [29] &quot;Cultivated&quot; &quot;Flooded&quot; [31] &quot;Urban&quot; &quot;Snow&quot; [33] &quot;Barren&quot; &quot;Water&quot; [35] &quot;Dominant.Land.Cover&quot; &quot;Tree.Cover&quot; I will write code to assign the right names. That way I won’t risk giving the wrong names to columns. # these are the annoying names oldcols &lt;- c(bioclimnames$name, &quot;USA1_alt&quot;) # better names newcols &lt;- c(bioclimnames$col, &quot;elevation&quot;) for (i in 1:length(oldcols)) names(allVars)[names(allVars) == oldcols[i]] &lt;- newcols[i] 4.7 Fix temperature in GBIF The temperature returned by GBIF is temperature x 10 so I will fix that. for (i in names(allVars)[stringr::str_detect(names(allVars), &quot;temp&quot;)]) { allVars[[i]] &lt;- allVars[[i]]/10 } 4.8 Save Finally, I will save allVars the stack of raster layers to a file. rf &lt;- raster::writeRaster(allVars, filename = &quot;data/allVars.grd&quot;, overwrite = TRUE) Plot. I use rf here because raster layers are associated with files on disk which need to be read. R Markdown doesn’t like reading from the temporary files sometimes. By using rf I direct it to use the data/allVars.grd file not a temporary file. plot(rf) "],
["sdm-data-frame.html", "Chapter 5 SDM Data Frame 5.1 Set-up 5.2 Variable values for presences 5.3 Background points 5.4 Make final data frame 5.5 Save", " Chapter 5 SDM Data Frame 5.1 Set-up This example will use the following libraries: library(raster) Load the shapefiles, Trillium data and variables raster stack created earlier. load(&quot;data/shapefiles.RData&quot;) load(&quot;data/trillium_presences.RData&quot;) allVars &lt;- raster::brick(&quot;data/allVars.grd&quot;) 5.2 Variable values for presences For fitting an SDM, I need a data frame where each row is a grid cell (in my NHVT raster) where the species has been observed (so presence = 1) and I have the variable value for that cell in the other columns. So the idea is to get the variable values for each of these observations. plot(nhvtshp, border = &quot;blue&quot;, axes = TRUE) plot(subset(trillium, species == &quot;Trillium grandiflorum&quot;), pch = 3, cex = 1, add = TRUE) plot(subset(trillium, species == &quot;Trillium undulatum&quot;), pch = 4, cex = 1, col = &quot;red&quot;, add = TRUE) plot(hbshp, add = TRUE, border = &quot;blue&quot;) EXCEPT that the variable data is on a grid while the observation data is points with lat/lon values. What I need is a data frame where each row is a grid cell where Trillium was observed (either once or many times) and the variable values for that cell. The following code gets you to that data frame. 5.2.1 Get the variable data I can create a data frame with the values of the variables where Trillium locations are using the extract() function. This will take a lat/lon value, figure out what cell that lat/lon pair is in, and return the variable values for that cell. The function takes a raster layer or stack of layers and the point location data (as a spatial points object) and returns the values for each layer in a column. I use cellnumbers=TRUE to return what cell that lat/lon pair is in. This will allow me to eliminate duplicates (observations from the same cell). trillVars &lt;- data.frame(raster::extract(allVars, trillium, cellnumbers = TRUE)) 5.2.2 Add on the species, pa, and lon/lat info I add on columns for the species name and “presence-absence”. trillVars$species &lt;- trillium$species trillVars$pa &lt;- 1 trillVars$lon &lt;- raster::xFromCell(r, trillVars$cells) trillVars$lat &lt;- raster::yFromCell(r, trillVars$cells) These are for grid cells. For later plotting, I might want the center of the cells so I’ll add that on. trillVars$lon &lt;- raster::xFromCell(r, trillVars$cells) trillVars$lat &lt;- raster::yFromCell(r, trillVars$cells) 5.2.3 Check for duplicates Duplicates means multiple occurrences assigned to the same cell. We can find this by seeing how many values in the cells column are duplicates. There are many. Our raster cells are 0.5 x 0.5 degrees (so 1km or so?). Trillium occurrences in that same cell (lat/lon values that are in the same 0.5 square grid) will have the same cell number so will be “duplicates”, meaning an occurrence in a cell where there is already another occurrence. # this is where both cell and species are the same dups &lt;- duplicated(cbind(trillVars$cells, trillVars$species)) sum(dups) [1] 478 I get rid of them by saying take the non-duplicated cell values. trillVars &lt;- trillVars[!dups, ] Now I have about half the number of lines of data. dim(trillVars) [1] 750 41 5.3 Background points SDMs for presence only data need a set of samples from the background area where it is possible that the species could have been observed. Technically, we’d want to weight this background by where searching was more likely or at least remove regions where it is impossible to observe the species. Trillium won’t be observed in urban areas and water bodies so we might want to create a mask of impossible areas not include background from there. But to keep things simple, for now I’ll just sample randomly from my NHVT bounding box. I will add on 5000 random background points by sampling from all the cells in allVars. I want to keep the lon/lat information (cell centers in this case). background &lt;- data.frame(raster::sampleRandom(allVars, size = 5000, cells = TRUE, xy = TRUE)) I need to fix the first three column names since I will be appending this data frame to the one above and that one uses cells as the cell column name. Also add on species name and presence-absence info. names(background)[1:3] &lt;- c(&quot;cells&quot;, &quot;lon&quot;, &quot;lat&quot;) background$pa &lt;- 0 To make it easier to create my training and testing datasets for the two species, I make two copies of the background with a different species value for each. background &lt;- rbind(background, background) background$species &lt;- c(rep(&quot;Trillium grandiflorum&quot;, 5000), rep(&quot;Trillium undulatum&quot;, 5000)) In order to bind this data frame to the presence one, I need the column names to also be in the same order. I need to fix that since sampleRandom() put the lat/lon columns in columns 2 and 3. The first line of code is selecting columns from background in the order shown after the , then I check that I did it right and the colnames are identical background &lt;- background[, colnames(trillVars)] identical(colnames(background), colnames(trillVars)) [1] TRUE 5.4 Make final data frame Now I bind this two data frames together for final data frame with occurrences, climatic data, and my background zeros. dat &lt;- rbind(trillVars, background) The rownames are annoying so I will set to NULL. rownames(dat) &lt;- NULL Now I have my final data frame with the following column names. colnames(dat) [1] &quot;cells&quot; &quot;mean.temp&quot; [3] &quot;temp.diurnal.range&quot; &quot;isotherm&quot; [5] &quot;temp.seasonality&quot; &quot;max.warm.temp&quot; [7] &quot;min.cold.temp&quot; &quot;temp.annual.range&quot; [9] &quot;mean.temp.wet.qtr&quot; &quot;mean.temp.dry.qtr&quot; [11] &quot;mean.temp.warm.qtr&quot; &quot;mean.temp.cold.qtr&quot; [13] &quot;total.precip&quot; &quot;precip.wet.month&quot; [15] &quot;precip.dry.month&quot; &quot;precip.seasonality&quot; [17] &quot;precip.wet.qtr&quot; &quot;precip.dry.qtr&quot; [19] &quot;precip.warm.qtr&quot; &quot;precip.cold.qtr&quot; [21] &quot;elevation&quot; &quot;slope&quot; [23] &quot;aspect&quot; &quot;Mixed.Needleleaf.Trees&quot; [25] &quot;Evergreen.Broadleaf.Trees&quot; &quot;Deciduous.Broadleaf.Trees&quot; [27] &quot;Mixed.Other.Trees&quot; &quot;Shrubs&quot; [29] &quot;Herbaceous&quot; &quot;Cultivated&quot; [31] &quot;Flooded&quot; &quot;Urban&quot; [33] &quot;Snow&quot; &quot;Barren&quot; [35] &quot;Water&quot; &quot;Dominant.Land.Cover&quot; [37] &quot;Tree.Cover&quot; &quot;species&quot; [39] &quot;pa&quot; &quot;lon&quot; [41] &quot;lat&quot; 5.5 Save I save the data frame. dat is a non-descriptive name and a little dangerous to use since the user might have dat already in their working environment. I will also make dat.und and dat.grand for the two Trillium species. There are some NAs in the slope aspect data which I will get rid of. dat.und &lt;- subset(dat, species == &quot;Trillium undulatum&quot;) dat.und &lt;- na.omit(dat.und) dat.grand &lt;- subset(dat, species == &quot;Trillium grandiflorum&quot;) dat.grand &lt;- na.omit(dat.grand) Save. save(dat, dat.und, dat.grand, file = &quot;data/sdm_data.RData&quot;) "],
["variable-correlation.html", "Chapter 6 Variable Correlation 6.1 Set-up 6.2 Variable correlation 6.3 Variance Inflation Factor 6.4 Save", " Chapter 6 Variable Correlation Including variables that are highly correlated is a big problem when doing regression analyses. You will get pairs of highly positive and negative effect size estimates and huge standard errors on your estimates. So we need to evaluate correlation and select a set of variables that is not terribly correlated. This example will use the following libraries. library(corrplot) library(usdm) library(stringr) library(raster) 6.1 Set-up Load the data for the SDMs prepared in earlier chapters. This loads dat.und with the climate data for each cell with presences of Trillium undulatum plus the background cells. load(&quot;data/sdm_data.RData&quot;) There are a few covariates that I know I don’t want. Specifically, the wet and dry qtr variables because that is summer in some cells and winter in others. Also I’ll exclude Snow since that is all 0 and Barren since that is almost all 0. And I’ll exclude cells and species since I don’t use those ever. Finally, dominant land cover is a categorical variable so I will exclude that (to make my life easier). dat.und &lt;- dat.und[, !stringr::str_detect(colnames(dat.und), &quot;[.]wet[.]&quot;)] dat.und &lt;- dat.und[, !stringr::str_detect(colnames(dat.und), &quot;[.]dry[.]&quot;)] dat.und &lt;- subset(dat.und, select = c(-cells, -species, -Dominant.Land.Cover, -Snow, -Barren)) 6.2 Variable correlation First I will use the corrplot package to look at correlation visually. tmpdat &lt;- subset(dat.und, select = stringr::str_detect(colnames(dat.und), &quot;precip&quot;)) varCor &lt;- cor(tmpdat, use = &quot;na.or.complete&quot;) corrplot::corrplot(varCor) Many of the temperature variables are very correlated. tmpdat &lt;- subset(dat.und, select = stringr::str_detect(colnames(dat.und), &quot;temp&quot;)) varCor &lt;- cor(tmpdat, use = &quot;na.or.complete&quot;) corrplot::corrplot(varCor) After exploring the models, I came up with the following set of not too correlated variables that still explain much of the variability in presence/absence. This set was selected by 1) considering what factors might influence a long-lived wildflower, 2) looking at the correlation plots, and 3) fitting different GAM models and looking at the effects curves for evidence of collinearity problems. The latter can be seen with curves that are mirrors of each other with high uncertainty (a hallmark of collinearity problems). But below I will also try variance inflation to select a set of non-collinear variables. envvars &lt;- c(&quot;mean.temp&quot;, &quot;temp.diurnal.range&quot;, &quot;temp.seasonality&quot;, &quot;precip.warm.qtr&quot;, &quot;precip.cold.qtr&quot;) That gets me a set of variables that are not so horribly correlated. tmpdat &lt;- dat.und[, envvars] varCor &lt;- cor(tmpdat, use = &quot;na.or.complete&quot;) corrplot::corrplot(varCor) And the variance inflation factors look ok. usdm::vif(tmpdat) Variables VIF 1 mean.temp 5.210197 2 temp.diurnal.range 1.500477 3 temp.seasonality 4.032876 4 precip.warm.qtr 5.607584 5 precip.cold.qtr 3.739644 plot(allVars[[envvars]]) 6.3 Variance Inflation Factor As an experiment, I will use variable inflation to select a set of non-correlated variables. This doesn’t try to use any biological reasoning about limiting factors for Trillium. It is just a statistical method to chose a set of uncorrelated variables. vifres &lt;- usdm::vifstep(subset(dat.und, select = c(-pa, -lon, -lat))) vifvars &lt;- as.character(vifres@results$Variables) vifvars [1] &quot;temp.diurnal.range&quot; &quot;temp.seasonality&quot; [3] &quot;mean.temp.warm.qtr&quot; &quot;precip.seasonality&quot; [5] &quot;precip.warm.qtr&quot; &quot;precip.cold.qtr&quot; [7] &quot;slope&quot; &quot;aspect&quot; [9] &quot;Evergreen.Broadleaf.Trees&quot; &quot;Deciduous.Broadleaf.Trees&quot; [11] &quot;Shrubs&quot; &quot;Herbaceous&quot; [13] &quot;Cultivated&quot; &quot;Flooded&quot; [15] &quot;Urban&quot; &quot;Water&quot; 6.4 Save I’ll test models with all variables and these subsets. topovars &lt;- c(&quot;elevation&quot;, &quot;slope&quot;, &quot;aspect&quot;) lcvars &lt;- c(&quot;Tree.Cover&quot;, &quot;mean.temp&quot;, &quot;precip.warm.qtr&quot;) envvars &lt;- c(&quot;mean.temp&quot;, &quot;temp.diurnal.range&quot;, &quot;temp.seasonality&quot;, &quot;precip.warm.qtr&quot;, &quot;precip.cold.qtr&quot;) minEnvVars &lt;- c(&quot;precip.warm.qtr&quot;, &quot;mean.temp&quot;, &quot;temp.diurnal.range&quot;) I’ll save because I’ll be using these variables across different SDM chapters. This code loads in my existing sdm_data file, records what objects are there (tmp is a character vector of objects in sdm_data), and then resaves with the original objects plus the ones I want to add. tmp &lt;- load(&quot;data/sdm_data.RData&quot;) save(topovars, lcvars, envvars, minEnvVars, vifvars, list = tmp, file = &quot;data/sdm_data.RData&quot;) "],
["training-and-testing-data.html", "Chapter 7 Training and Testing Data 7.1 Presence train/test 7.2 Background train/test 7.3 Training data 7.4 Create many datasets 7.5 Save", " Chapter 7 Training and Testing Data Our dat, dat.und and dat.grand data frames have all our data. But when we fit models to data, we need to hold out some data for testing the fit. This is also called cross-validation. The jargon used is folds where each fold is the random sample of data that you will test against. So if we do k-fold cross-validation where k=5. That means we randomly assign our data to 5 groups (1 to 5). We do 5 fits. The first one will use group 1 as the testing data, and fit the model to the other data. The second one will use group 2 as the testing data and fit to the rest. Etc. That ensure that you test against different data each time. Another way to do this test is to randomly select 1/k proportion of your data to use for testing and repeat that many times to create many test/train data sets. I am going to use this approach. This chapter will use the following libraries. library(dismo) Load the data. This will output the names that are loaded. datnames &lt;- load(&quot;data/sdm_data.RData&quot;) 7.1 Presence train/test We set up a training set of presence data and a test set. kfold is just a function to randomly assign the data to k groups. I want just the presence data so will subset to pa column equal 1. I will call the presence only data presdat. set.seed(10) presdat &lt;- subset(dat.und, pa == 1) group &lt;- dismo::kfold(presdat, k = 5) # 5 groups = 20 test/80 train split The testing data will be group==1 and training data will be the rest. pres_train &lt;- presdat[group != 1, ] pres_test &lt;- presdat[group == 1, ] 7.2 Background train/test We repeat the process above for the background data (the pa=0). bgdat &lt;- subset(dat.und, pa == 0) group &lt;- dismo::kfold(bgdat, k = 5) backg_train &lt;- bgdat[group != 1, ] backg_test &lt;- bgdat[group == 1, ] 7.3 Training data We make separate presence and background train/test sets for evaluation purposes later. But for fitting we need a data frame with both train data sets (presence and background) together. traindat &lt;- rbind(pres_train, backg_train) 7.4 Create many datasets The above code would create just one dataset, but we want to create many since we want to see how/if the model changes with a different training set. I’ll create a list and save the data there. I will run through the code above and assign my train/test datasets to a list. I need to save traindat used in the model fitting and pres_test and backg_test used in the evaluation functions. Note, this is incredibly memory inefficient. I really just need to store dat.und and group, but for convenience sake, I am pre-making whole datasets and storing those. If the dataset were larger, I could not do this. traindatlist &lt;- list() n &lt;- 20 for (i in 1:n) { presdat &lt;- subset(dat.und, pa == 1) group &lt;- dismo::kfold(presdat, k = 5) # 5 groups = 20 test/80 train split pres_train &lt;- presdat[group != 1, ] pres_test &lt;- presdat[group == 1, ] bgdat &lt;- subset(dat.und, pa == 0) group &lt;- dismo::kfold(bgdat, k = 5) backg_train &lt;- bgdat[group != 1, ] backg_test &lt;- bgdat[group == 1, ] traindatlist[[i]] &lt;- list(traindat = rbind(pres_train, backg_train), pres_test = pres_test, backg_test = backg_test) } 7.5 Save I’ll add this to the sdm_data data file. save(traindatlist, list = datnames, file = &quot;data/sdm_data.RData&quot;) "],
["sdms-glm.html", "Chapter 8 SDMs – GLM 8.1 Set-up 8.2 Fit GLM 8.3 Predictions 8.4 Model Evaluation 8.5 More GLM fits 8.6 Variable selection 8.7 Model Comparison 8.8 Model comparison table (first data set) 8.9 Save", " Chapter 8 SDMs – GLM In a linear regression, we look to explain the value of a response variable with a linear combination of our explanatory variables. For example, \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\] In the case of the presence/background SDM, our observations are 0 or 1. We do logistic regression and our \\(y\\) is the probability of an observation (probability of ‘success’ aka a 1). We are trying to find a set of variables and \\(\\beta\\)s where cells with 1s are predicted to have high probability of observation and cells with no observations have low probability. Keep in mind that we are using background, not actually absences (0s). It’s important when doing this kind of analysis to not have too much correlation between your explanatory variables. We already dealt with selecting a set of uncorrelated variables in the chapter on variable correlation, but we’ll see a couple more approaches in this chapter too. 8.1 Set-up This chapter will use the following libraries. library(biomod2) library(MASS) library(ggplot2) library(ecospat) library(sp) library(raster) 8.1.1 Load the data Load the shapefiles, Trillium data and variables raster stack created earlier. # The VT, NH and HB outlines load(&quot;data/shapefiles.RData&quot;) # The original observation data lat/lon load(&quot;data/trillium_presences.RData&quot;) # All the data needed to fit the SDM The grid cells with # presences and background grid cells w predictors The # variables that I&#39;ll use for various models load(&quot;data/sdm_data.RData&quot;) # Raster of all predictors allVars &lt;- raster::brick(&quot;data/allVars.grd&quot;) 8.1.2 First dataset For this first fit, I will use the first dataset in my training/test list. traindat &lt;- traindatlist[[1]]$traindat pres_test &lt;- traindatlist[[1]]$pres_test backg_test &lt;- traindatlist[[1]]$backg_test 8.1.3 Model list I will store my fits to a list so that I can easily compare them. modellist &lt;- list() 8.2 Fit GLM Fit a generalized linear model with all the environmental variables. pa ~ . is a model formula. To the left of ~ is the response, in this case presence/absence. To the right of ~ are the predictor variables. . is saying use all variables in the data. To use that shortcut, I need to pass in a data frame with only pa (my response variable) and the predictor variables that I want to use. Thus for the environmental variables model, I pass in traindat[,c(\"pa\", envvars)] which is a data frame with pa and envvars selected. binomial(link = \"logit\") is what we specify for logistic regression (modeling 0/1 data). fit &lt;- stats::glm(formula = pa ~ ., family = binomial(link = &quot;logit&quot;), data = traindat[, c(&quot;pa&quot;, envvars)]) Save to my model list. modellist[[&quot;glmEnv&quot;]] &lt;- list(model = fit, name = &quot;glmEnv&quot;, desc = &quot;GLM - environmental variables&quot;, terms = envvars) 8.3 Predictions To plot predictions, use the predict() function. We need to pass in the raster stack of the predictor variables and the model. I want to use type=response in order to get the probabilities (which is what a binomial fit returns). pm &lt;- predict(allVars, fit, type = &quot;response&quot;) Now we can plot the prediction. plot(pm) plot(nhvtshp, add = TRUE, border = &quot;blue&quot;) Because I will be making similar plots, I will make a function for my plots so they all look the same. pm.plot &lt;- function(x, main = &quot;&quot;, scale.min = 0, scale.max = 1, ..., mar = c(5, 4, 4, 5)) { par(mar = mar) plot(x, main = main, breaks = seq(scale.min, scale.max, (scale.max - scale.min)/10), col = rev(terrain.colors(11)), xlab = &quot;Longitude&quot;, ylab = &quot;Latitude&quot;, cex = 0.75, ...) plot(nhvtshp, add = TRUE, border = &quot;blue&quot;) plot(hbshp, add = TRUE) } Now make the prediction plot with this function. pm.plot(pm, main = &quot;Environmental Variables&quot;) Let’s zoom in on Hubbard Brook. xlims &lt;- c(-71.9, -71.6) ylims &lt;- c(43.875, 44) pm.plot(pm, main = &quot;Environmental Variables&quot;, xlim = xlims, ylim = ylims, scale.max = 0.5) 8.4 Model Evaluation 8.4.1 AUC Curves This is a plot of how well the model predicts presence and background in the test data (so not the data you used to fit the model). A value of 0.5 is random. Values closer to 1 mean the model predicts better than random. 0.7 would be a mediocre model. This metric is sensitive to what you choose as background and how much background points you have. The dismo::evaluate() function will calculate this metric for us. It needs the presence and background test data and the model fit. erf &lt;- dismo::evaluate(pres_test, backg_test, model = fit) plot(erf, &quot;ROC&quot;) 8.4.2 Boyce Index The Boyce Index looks just at the presence data. We want this curve to go steadily up from left to right. The ecospat::ecospat.boyce() computes this metric. predict(allVars, fit, type = &quot;response&quot;) ecospat::ecospat.boyce(pm, cbind(pres_test$lon, pres_test$lat)) Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used Our model with environmental variables is ok (it goes up as x increases). 8.4.3 Response curves This shows the relationship between the preditor variable and the probability of presence. rp &lt;- biomod2::response.plot2(models = c(&quot;fit&quot;), Data = traindat, show.variables = envvars, fixed.var.metric = &quot;mean&quot;, use.formal.names = TRUE) Warning in which.max(sum_level): NAs introduced by coercion Since rp is a data frame in long form, we can also use ggplot to plot. p &lt;- ggplot(rp, aes(x = expl.val, y = pred.val, lty = pred.name)) + geom_line() + ylab(&quot;prob of occ&quot;) + xlab(&quot;&quot;) + facet_wrap(~expl.name, scales = &quot;free_x&quot;) + ggtitle(&quot;Environmental variables&quot;) p 8.4.4 Variable importance Let’s look at the variable importance. This is a measure of how much each variable singly impacts the fit. varimp &lt;- biomod2::variables_importance(fit, data = traindat)$mat varimp[varimp &gt; 0.01, ] temp.diurnal.range temp.seasonality precip.warm.qtr precip.cold.qtr 0.026181 0.349757 1.000000 0.090418 8.5 More GLM fits Now I repeat the model fitting code for the other sets of variables and store to my model list. Careful, this is not memory efficient and modellist will be large. I am doing this for convenience since my region is not too big. If I had a big region, the model objects would be large and I would need to be careful with memory. 8.5.1 Topography only This is the model with just topography. vars &lt;- topovars vars [1] &quot;elevation&quot; &quot;slope&quot; &quot;aspect&quot; The only change is that the data are now traindat[,c(\"pa\", vars)] with vars=topovars. fit &lt;- stats::glm(formula = pa ~ ., family = binomial(link = &quot;logit&quot;), data = traindat[, c(&quot;pa&quot;, vars)]) modellist[[&quot;glmTopo&quot;]] &lt;- list(model = fit, name = &quot;glmTopo&quot;, desc = &quot;GLM - topographical variables&quot;, terms = vars) 8.5.2 Tree Cover only This is the model with tree cover and a few environmental variables. vars &lt;- lcvars vars [1] &quot;Tree.Cover&quot; &quot;mean.temp&quot; &quot;precip.warm.qtr&quot; fit &lt;- stats::glm(formula = pa ~ ., family = binomial(link = &quot;logit&quot;), data = traindat[, c(&quot;pa&quot;, vars)]) modellist[[&quot;glmLC&quot;]] &lt;- list(model = fit, name = &quot;glmLC&quot;, desc = &quot;GLM - LC variables&quot;, terms = vars) 8.5.3 VIF variables These were variables selected by the Variance Inflation step search function. vars &lt;- vifvars vars [1] &quot;temp.diurnal.range&quot; &quot;temp.seasonality&quot; [3] &quot;mean.temp.warm.qtr&quot; &quot;precip.seasonality&quot; [5] &quot;precip.warm.qtr&quot; &quot;precip.cold.qtr&quot; [7] &quot;slope&quot; &quot;aspect&quot; [9] &quot;Evergreen.Broadleaf.Trees&quot; &quot;Deciduous.Broadleaf.Trees&quot; [11] &quot;Shrubs&quot; &quot;Herbaceous&quot; [13] &quot;Cultivated&quot; &quot;Flooded&quot; [15] &quot;Urban&quot; &quot;Water&quot; fit &lt;- stats::glm(formula = pa ~ ., family = binomial(link = &quot;logit&quot;), data = traindat[, c(&quot;pa&quot;, vars)]) Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred modellist[[&quot;glmVIF&quot;]] &lt;- list(model = fit, name = &quot;glmVIF&quot;, desc = &quot;GLM - all VIF variables&quot;, terms = vars) 8.5.4 Small set of environmental variables vars &lt;- minEnvVars vars [1] &quot;precip.warm.qtr&quot; &quot;mean.temp&quot; &quot;temp.diurnal.range&quot; fit &lt;- stats::glm(formula = pa ~ ., family = binomial(link = &quot;logit&quot;), data = traindat[, c(&quot;pa&quot;, vars)]) modellist[[&quot;glmMinEnv&quot;]] &lt;- list(model = fit, name = &quot;glmMinEnv&quot;, desc = &quot;GLM - minimal environmental variables&quot;, terms = vars) 8.6 Variable selection The model with the VIF variables still has a lot of variables. vifvars [1] &quot;temp.diurnal.range&quot; &quot;temp.seasonality&quot; [3] &quot;mean.temp.warm.qtr&quot; &quot;precip.seasonality&quot; [5] &quot;precip.warm.qtr&quot; &quot;precip.cold.qtr&quot; [7] &quot;slope&quot; &quot;aspect&quot; [9] &quot;Evergreen.Broadleaf.Trees&quot; &quot;Deciduous.Broadleaf.Trees&quot; [11] &quot;Shrubs&quot; &quot;Herbaceous&quot; [13] &quot;Cultivated&quot; &quot;Flooded&quot; [15] &quot;Urban&quot; &quot;Water&quot; Let’s try stepwise variable selection using AIC as criteria. glmStart &lt;- glm(pa ~ ., family = binomial(link = &quot;logit&quot;), data = traindat[, c(&quot;pa&quot;, vifvars)]) glmStep &lt;- MASS::stepAIC(object = glmStart, scope = pa ~ ., data = traindat[, c(&quot;pa&quot;, vifvars)], direction = &quot;both&quot;, trace = -1, k = 2, control = glm.control(maxit = 500)) # Save modellist[[&quot;glmStep&quot;]] &lt;- list(model = glmStep, name = &quot;glmStep&quot;, desc = &quot;GLM - variables selected w step AIC&quot;, terms = attr(glmStep$terms, &quot;term.labels&quot;)) This model has fewer terms: attr(glmStep$terms, &quot;term.labels&quot;) [1] &quot;temp.seasonality&quot; &quot;mean.temp.warm.qtr&quot; [3] &quot;precip.seasonality&quot; &quot;precip.warm.qtr&quot; [5] &quot;precip.cold.qtr&quot; &quot;slope&quot; [7] &quot;Deciduous.Broadleaf.Trees&quot; &quot;Herbaceous&quot; [9] &quot;Cultivated&quot; &quot;Urban&quot; [11] &quot;Water&quot; 8.7 Model Comparison 8.7.1 AIC AIC is a measure of model fit penalized by how many parameters are being fit. It is a model fit metric that helps you not overfit a model (use too many parameters). Smaller AIC is better. The actual value is unimportant rather you compare AICs within a set. Things to keep in mind. The data must be identical in the models being compared. Best not to compare AICs from different functions unless you are sure they are showing the same values. Some functions drop a constant, since only differences between models matter. But if you are comparing AICs from models fit with different functions, it critical that the constant is not dropped. This is less of a problem now as function writers are more careful, but just be aware. Here are the AICs so far. lapply means apply a function to all objects in a list. AIC() is the function that returns the AIC for a model. sort(unlist(lapply(modellist, function(x) { AIC(x$model) }))) glmStep glmVIF glmLC glmEnv glmTopo glmMinEnv 2648.513 2656.807 2720.935 2749.253 2775.568 2780.547 8.7.2 AUC curves First I’ll make a data frame. dfb &lt;- data.frame(x = NULL, y = NULL, model = NULL) for (i in names(modellist)) { erf &lt;- dismo::evaluate(pres_test, backg_test, model = modellist[[i]]$model) a &lt;- data.frame(x = erf@FPR, y = erf@TPR, model = i, title = paste0(i, &quot; AUC=&quot;, round(erf@auc, digits = 2))) dfb &lt;- rbind(dfb, a) } Now I can use that data frame with ggplot. p &lt;- ggplot(dfb, aes(x = x, y = y)) + geom_point(col = &quot;red&quot;) + ylab(&quot;True positive rate&quot;) + xlab(&quot;False positive rate&quot;) p + facet_wrap(~title) + ggtitle(&quot;Evaluation of the false/positive performance&quot;) + geom_abline(intercept = 0) 8.7.3 Boyce Index Let’s check model quality using the Boyce Index. First I will add the index to all my model lists. Since I am doing model predictions, I’ll save that to a raster stack for plotting later. This takes a long time, so I skip if I’ve already added the index to the list. for (i in names(modellist)) { pm &lt;- predict(allVars, modellist[[i]]$model, type = &quot;response&quot;) if (i == names(modellist)[1]) pm.stack &lt;- pm else pm.stack &lt;- raster::stack(pm.stack, pm) if (&quot;bindex&quot; %in% names(modellist[[i]])) next bindex &lt;- ecospat::ecospat.boyce(pm, cbind(pres_test$lon, pres_test$lat), PEplot = FALSE) modellist[[i]]$bindex &lt;- bindex } Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used names(pm.stack) &lt;- names(modellist) Then I’ll make a data frame. dfb &lt;- data.frame(x = NULL, y = NULL, model = NULL) for (i in names(modellist)) { bi &lt;- modellist[[i]]$bindex a &lt;- data.frame(y = bi$F.ratio, x = bi$HS, model = i) dfb &lt;- rbind(dfb, a) } dfb$observed &lt;- &quot;yes&quot; dfb$observed[dfb$y == 0] &lt;- &quot;no&quot; Now I can use that data frame with ggplot. p &lt;- ggplot(dfb, aes(x = x, y = y)) + geom_point(aes(col = observed)) + ylab(&quot;Boyce Index&quot;) + xlab(&quot;Suitability&quot;) p + facet_wrap(~model) + ggtitle(&quot;Evaluation of the test data performance&quot;) 8.7.4 Response curves We can compare the response curves for models which have the same variables. glmEnv &lt;- modellist[[&quot;glmEnv&quot;]]$model glmLC &lt;- modellist[[&quot;glmLC&quot;]]$model rp &lt;- biomod2::response.plot2(models = c(&quot;glmEnv&quot;, &quot;glmLC&quot;), Data = traindat, show.variables = envvars, fixed.var.metric = &quot;mean&quot;, plot = FALSE, use.formal.names = TRUE) Warning in which.max(sum_level): NAs introduced by coercion The models don’have all the variables. I will put NAs if the model doesn’t have that variable. rp$include &lt;- apply(rp, 1, function(x) { x[2] %in% modellist[[x[4]]]$terms }) rp$pred.val[!rp$include] &lt;- NA gg.rp &lt;- ggplot(rp, aes(x = expl.val, y = pred.val, col = pred.name)) + geom_line(na.rm = TRUE) + ylab(&quot;prob of occ&quot;) + xlab(&quot;&quot;) + facet_wrap(~expl.name, scales = &quot;free_x&quot;) + ggtitle(&quot;Environmental variables&quot;) print(gg.rp) 8.7.5 Predictions Unfortunately when it plots a stack, the border shapes don’t show up. pm.plot(pm.stack, main = names(pm.stack)) Let’s zoom in on Hubbard Brook. xlims &lt;- c(-71.9, -71.6) ylims &lt;- c(43.875, 44) pm.plot(pm.stack, main = names(pm.stack), xlim = xlims, ylim = ylims, scale.max = 0.5) 8.8 Model comparison table (first data set) Compare AICs, Spearman Correlation for the models with the first data set (in traindatlist). df &lt;- data.frame(name = unlist(lapply(modellist, function(x) { x$name })), Spearman = unlist(lapply(modellist, function(x) { x$bindex$Spearman.cor })), AUC = unlist(lapply(modellist, function(x) { dismo::evaluate(pres_test, backg_test, model = x$model)@auc })), AIC = unlist(lapply(modellist, function(x) { AIC(x$model) }))) df$delAIC &lt;- df$AIC - min(df$AIC) df &lt;- df[order(df$AIC), ] knitr::kable(df, row.names = FALSE) name Spearman AUC AIC delAIC glmStep 0.939 0.7425372 2648.513 0.000000 glmVIF 0.948 0.7430992 2656.807 8.294002 glmLC 0.902 0.7117438 2720.935 72.421988 glmEnv 0.811 0.7183471 2749.253 100.740644 glmTopo 0.856 0.7145455 2775.568 127.055216 glmMinEnv 0.865 0.6961157 2780.547 132.034099 8.9 Save Save so I can use for the next chapter on GAMs. I will also save the function I made for plotting. save(pm.plot, modellist, file = &quot;modellist.RData&quot;) "],
["sdms-gam.html", "Chapter 9 SDMs – GAM 9.1 Set-up 9.2 Fit GAM 9.3 Effect size curves 9.4 More GAM fits 9.5 Model Comparison 9.6 AUCs 9.7 Boyce Index - Spearman 9.8 Hubbard Brook comparisons 9.9 Model comparison table (first data set) 9.10 Save", " Chapter 9 SDMs – GAM A generalized additive model (GAM) is similar to a linear regression model except that the relationship between the response variable and explanatory variables is flexible. For example, \\[y = \\alpha + s_1(x_1) + s_2(x_2) + \\epsilon\\] where \\(s()\\) is a function. The code for fitting the GAMs will be similar in structure to the code for fitting the GLMs and the same evaluation metrics can be calculated. 9.1 Set-up This chapter will use the following libraries. library(biomod2) library(gam) library(ggplot2) library(ecospat) library(sp) library(raster) 9.1.1 Load the data and GLM models Load the shapefiles, Trillium data and variables raster stack created earlier. # The VT, NH and HB outlines load(&quot;data/shapefiles.RData&quot;) # The original observation data lat/lon load(&quot;data/trillium_presences.RData&quot;) # All the data needed to fit the SDM The grid cells with # presences and background grid cells w predictors The # variables that I&#39;ll use for various models load(&quot;data/sdm_data.RData&quot;) # Raster of all predictors allVars &lt;- raster::brick(&quot;data/allVars.grd&quot;) Load the model list from the GLM chapter plus the pm.plot function. load(&quot;modellist.RData&quot;) 9.1.2 First data set For the first fit, I use the first data set in my training/test list. traindat &lt;- traindatlist[[1]]$traindat pres_test &lt;- traindatlist[[1]]$pres_test backg_test &lt;- traindatlist[[1]]$backg_test 9.2 Fit GAM The GAM formula in R looks like response ~ s(var1, sm) + s(var2, sm). The s() is the spline function and allows the response to be non-linear. The second number, sm is the amount of smoothing and the default way you specify this is different for the gam::gam() function versus the mcgv::gam() function. Here I use gam::gam() and use the df argument (default). df=1 would be linear. I will write a function to make my formula for the gam() call. That way I don’t have to make it manually. gamfm &lt;- function(x, df, f = NULL) { if (length(f) != 0) x &lt;- x[x != f] fm &lt;- paste0(&quot;s(&quot;, x, &quot;, &quot;, df, &quot;)&quot;, collapse = &quot; + &quot;) if (length(f) != 0) { ff &lt;- paste0(f, collapse = &quot; + &quot;) fm &lt;- paste(fm, ff, sep = &quot;+&quot;) } fm &lt;- as.formula(paste(&quot;pa ~&quot;, fm)) return(fm) } For the GAM model with less smoothing (df=2), the formula is gamfm(envvars, 2) pa ~ s(mean.temp, 2) + s(temp.diurnal.range, 2) + s(temp.seasonality, 2) + s(precip.warm.qtr, 2) + s(precip.cold.qtr, 2) &lt;environment: 0x7fda3d918d50&gt; Notice how the variables are in a s() function. That is the spline function. Because I am specifying what variables to use in the model formula, I don’t need to subset the training data. I can just pass the whole data frame in. gamEnv2 &lt;- gam::gam(formula = gamfm(envvars, 2), data = traindat, family = &quot;binomial&quot;) For the more flexible model, I use gamEnv4 &lt;- gam::gam(formula = gamfm(envvars, 4), data = traindat, family = &quot;binomial&quot;) Save models. mod &lt;- paste0(&quot;gamEnv&quot;, c(2, 4)) desc &lt;- paste0(&quot;GAM - Environmental variables df=&quot;, c(2, 4)) for (i in 1:2) { pm &lt;- predict(allVars, get(mod[i]), type = &quot;response&quot;) bindex &lt;- ecospat::ecospat.boyce(pm, cbind(pres_test$lon, pres_test$lat), PEplot = FALSE) modellist[[mod[i]]] &lt;- list(model = get(mod[i]), name = mod[i], desc = desc[i], terms = envvars, bindex = bindex) } Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used We can compute the same output and diagnostics as we did for GLMs. 9.2.1 Predictions pm &lt;- predict(allVars, gamEnv4, type = &quot;response&quot;) pm.plot(pm) Let’s zoom in on Hubbard Brook. xlims &lt;- c(-71.9, -71.6) ylims &lt;- c(43.875, 44) pm.plot(pm, main = &quot;Environmental Variables&quot;, xlim = xlims, ylim = ylims, scale.max = 0.5) 9.2.2 Response curves This shows the relationship between the predictor variable and the probability of presence. rp &lt;- biomod2::response.plot2(models = c(&quot;gamEnv4&quot;, &quot;gamEnv2&quot;), Data = traindat, show.variables = envvars, fixed.var.metric = &quot;mean&quot;, use.formal.names = TRUE) Warning in which.max(sum_level): NAs introduced by coercion Since rp is a data frame in long form, we can also use ggplot to plot. p &lt;- ggplot(rp, aes(x = expl.val, y = pred.val, lty = pred.name)) + geom_line() + ylab(&quot;prob of occ&quot;) + xlab(&quot;&quot;) + facet_wrap(~expl.name, scales = &quot;free_x&quot;) + ggtitle(&quot;Environmental variables&quot;) p 9.2.3 AUC Curves erf &lt;- dismo::evaluate(pres_test, backg_test, model = gamEnv4) plot(erf, &quot;ROC&quot;) 9.2.4 Boyce Index predict(allVars, gamEnv4, type = &quot;response&quot;) ecospat::ecospat.boyce(pm, cbind(pres_test$lon, pres_test$lat)) Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used 9.2.5 Variable importance This is a measure of how much each variable singly impacts the fit. varimp &lt;- biomod2::variables_importance(gamEnv4, data = traindat)$mat varimp[varimp &gt; 0.01, ] mean.temp temp.diurnal.range temp.seasonality precip.warm.qtr 0.161965 0.102880 0.360015 0.336038 precip.cold.qtr 0.158276 9.3 Effect size curves Let’s look at the GAM effect size curves for the model with 5 environmental variables. The effect curves are non-linear. po &lt;- gam:::preplot.Gam(gamEnv4, terms = attr(terms(gamEnv4), &quot;term.labels&quot;)) dfenv &lt;- data.frame(x = NULL, y = NULL, se = NULL, variable = NULL) for (i in names(po)) { vname &lt;- stringr::str_replace(i, &quot;s[(]&quot;, &quot;&quot;) vname &lt;- stringr::str_replace(vname, &quot;, 4[)]&quot;, &quot;&quot;) a &lt;- data.frame(x = po[[i]]$x, y = po[[i]]$y, se = po[[i]]$se.y, variable = vname) dfenv &lt;- rbind(dfenv, a) } p &lt;- ggplot(dfenv, aes(x = x, y = y)) + geom_line() + geom_ribbon(aes(ymin = y + 2 * se, ymax = y - 2 * se), col = &quot;grey&quot;, alpha = 0.5) + ylab(&quot;effect size&quot;) p + facet_wrap(~variable, scales = &quot;free&quot;) 9.4 More GAM fits Now I repeat the model fitting code for the other sets of variables and store to my model list. 9.4.1 Topographical only model gamTopo2 &lt;- gam::gam(formula = gamfm(topovars, 2), data = traindat, family = &quot;binomial&quot;) gamTopo4 &lt;- gam::gam(formula = gamfm(topovars, 4), data = traindat, family = &quot;binomial&quot;) Save models. mod &lt;- paste0(&quot;gamTopo&quot;, c(2, 4)) desc &lt;- paste0(&quot;GAM - Topo variables df=&quot;, c(2, 4)) for (i in 1:2) { pm &lt;- predict(allVars, get(mod[i]), type = &quot;response&quot;) bindex &lt;- ecospat::ecospat.boyce(pm, cbind(pres_test$lon, pres_test$lat), PEplot = FALSE) modellist[[mod[i]]] &lt;- list(model = get(mod[i]), name = mod[i], desc = desc[i], terms = topovars, bindex = bindex) } Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used 9.4.2 Land cover model gamLC2 &lt;- gam::gam(formula = gamfm(lcvars, 2), data = traindat, family = &quot;binomial&quot;) gamLC4 &lt;- gam::gam(formula = gamfm(lcvars, 4), data = traindat, family = &quot;binomial&quot;) Save models. mod &lt;- paste0(&quot;gamLC&quot;, c(2, 4)) desc &lt;- paste0(&quot;GAM - Tree Cover variables df=&quot;, c(2, 4)) for (i in 1:2) { pm &lt;- predict(allVars, get(mod[i]), type = &quot;response&quot;) bindex &lt;- ecospat::ecospat.boyce(pm, cbind(pres_test$lon, pres_test$lat), PEplot = FALSE) modellist[[mod[i]]] &lt;- list(model = get(mod[i]), name = mod[i], desc = desc[i], terms = lcvars, bindex = bindex) } Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used 9.4.3 Minimal GAM Model gamEnvMin &lt;- gam::gam(formula = gamfm(minEnvVars, 4), data = traindat, family = &quot;binomial&quot;) pm &lt;- predict(allVars, gamEnvMin, type = &quot;response&quot;) bindex &lt;- ecospat::ecospat.boyce(pm, cbind(pres_test$lon, pres_test$lat), PEplot = FALSE) Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used modellist[[&quot;gamEnvMin&quot;]] &lt;- list(model = gamEnvMin, name = &quot;gamEnvMin&quot;, desc = &quot;GAM - Minimal&quot;, terms = minEnvVars, bindex = bindex) 9.4.4 VIF Vars gamVIF2 &lt;- gam::gam(formula = gamfm(vifvars, 2), data = traindat, family = &quot;binomial&quot;) gamVIF4 &lt;- gam::gam(formula = gamfm(vifvars, 4), data = traindat, family = &quot;binomial&quot;) Save models. mod &lt;- paste0(&quot;gamVIF&quot;, c(2, 4)) desc &lt;- paste0(&quot;GAM - VIF variables df=&quot;, c(2, 4)) for (i in 1:2) { pm &lt;- predict(allVars, get(mod[i]), type = &quot;response&quot;) bindex &lt;- ecospat::ecospat.boyce(pm, cbind(pres_test$lon, pres_test$lat), PEplot = FALSE) modellist[[mod[i]]] &lt;- list(model = get(mod[i]), name = mod[i], desc = desc[i], terms = vifvars, bindex = bindex) } Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used Warning in if (class(obs) == &quot;data.frame&quot; | class(obs) == &quot;matrix&quot;) {: the condition has length &gt; 1 and only the first element will be used 9.5 Model Comparison 9.5.1 Predictions Compare the topo, LC and environmenal predictions. pg.topo &lt;- predict(allVars, gamTopo4, type = &quot;response&quot;) pg.env &lt;- predict(allVars, gamEnv4, type = &quot;response&quot;) pg.lc &lt;- predict(allVars, gamLC4, type = &quot;response&quot;) pg.min &lt;- predict(allVars, gamEnvMin, type = &quot;response&quot;) Now make the prediction plots. par(mfrow = c(2, 2)) mar &lt;- c(0, 0, 2, 0) pm.plot(pg.topo, main = &quot;Topographical Variables&quot;, legend = FALSE, axes = FALSE, box = FALSE, mar = mar) pm.plot(pg.env, main = &quot;Environmental Variables&quot;, legend = FALSE, axes = FALSE, box = FALSE, mar = mar) pm.plot(pg.lc, main = &quot;Tree Cover Variables&quot;, legend = FALSE, axes = FALSE, box = FALSE, mar = mar) pm.plot(pg.min, main = &quot;Three Env Variables&quot;, legend = FALSE, axes = FALSE, box = FALSE, mar = mar) 9.6 AUCs aucs &lt;- unlist(lapply(modellist, function(x) { dismo::evaluate(pres_test, backg_test, model = x$model)@auc })) sort(aucs) glmMinEnv glmLC glmTopo glmEnv gamTopo2 gamEnvMin gamLC2 gamTopo4 0.6961157 0.7117438 0.7145455 0.7183471 0.7203802 0.7215950 0.7223140 0.7281488 gamLC4 glmStep glmVIF gamEnv2 gamEnv4 gamVIF2 gamVIF4 0.7331653 0.7425372 0.7430992 0.7521983 0.7664959 0.7701322 0.7897686 9.7 Boyce Index - Spearman Let’s look at the Spearman correlations from the Boyce Index. bis &lt;- unlist(lapply(modellist, function(x) { x$bindex$Spearman.cor })) sort(bis) gamLC4 glmEnv gamTopo4 gamEnvMin glmTopo glmMinEnv gamTopo2 gamLC2 0.777 0.811 0.832 0.832 0.856 0.865 0.873 0.873 glmLC gamEnv4 gamVIF2 glmStep gamEnv2 gamVIF4 glmVIF 0.902 0.904 0.930 0.939 0.939 0.939 0.948 Compare some of the Boyce Index plots. dfb &lt;- data.frame(x = NULL, y = NULL, model = NULL) for (i in c(&quot;gamEnv4&quot;, &quot;gamLC4&quot;, &quot;gamEnvMin&quot;, &quot;gamVIF4&quot;)) { bi &lt;- modellist[[i]]$bindex a &lt;- data.frame(y = bi$F.ratio, x = bi$HS, model = i) dfb &lt;- rbind(dfb, a) } dfb$observed &lt;- &quot;yes&quot; dfb$observed[dfb$y == 0] &lt;- &quot;no&quot; p &lt;- ggplot(dfb, aes(x = x, y = y)) + geom_point(aes(col = observed)) + ylab(&quot;Boyce Index&quot;) + xlab(&quot;Suitability&quot;) p + facet_wrap(~model, scales = &quot;free_y&quot;) + ggtitle(&quot;Evaluation of the test data performance&quot;) 9.8 Hubbard Brook comparisons Let’s zoom in on Hubbard Brook. The observations to the far right are next to the labs. The GLM makes the ridges (boundary) much higher in suitability than the lower elevation brook bottom (center). The model with tree cover also makes the ridge more suitable than the lower elevation. par(mfrow = c(2, 2)) xlims &lt;- c(-71.9, -71.6) ylims &lt;- c(43.875, 44) mar &lt;- c(0, 0, 3, 0) for (i in c(&quot;gamEnvMin&quot;, &quot;gamEnv4&quot;, &quot;gamLC4&quot;, &quot;gamTopo4&quot;)) { pg &lt;- predict(allVars, modellist[[i]]$model, type = &quot;response&quot;) pm.plot(pg, main = i, xlim = xlims, ylim = ylims, scale.max = 0.5, box = FALSE, axes = FALSE, legend = FALSE, mar = mar) } Tree composition in Hubbard Brook also tracks these environmental conditions it looks like. 9.8.1 Response curves We can compare the response curves for models which have the same variables. rp &lt;- biomod2::response.plot2(models = c(&quot;gamEnv4&quot;, &quot;gamLC4&quot;, &quot;gamVIF4&quot;), Data = traindat, show.variables = envvars, fixed.var.metric = &quot;mean&quot;, plot = FALSE, use.formal.names = TRUE) Warning in which.max(sum_level): NAs introduced by coercion The models don’have all the variables. I will put NAs if the model doesn’t have that variable. rp$include &lt;- apply(rp, 1, function(x) { x[2] %in% modellist[[x[4]]]$terms }) rp$pred.val[!rp$include] &lt;- NA gg.rp &lt;- ggplot(rp, aes(x = expl.val, y = pred.val, col = pred.name)) + geom_line(na.rm = TRUE) + ylab(&quot;prob of occ&quot;) + xlab(&quot;&quot;) + facet_wrap(~expl.name, scales = &quot;free_x&quot;) + ggtitle(&quot;Environmental variables&quot;) print(gg.rp) 9.9 Model comparison table (first data set) Compare AICs, Spearman Correlation for the models with the first data set (in traindatlist). df &lt;- data.frame(name = unlist(lapply(modellist, function(x) { x$name })), Spearman = unlist(lapply(modellist, function(x) { x$bindex$Spearman.cor })), AUC = unlist(lapply(modellist, function(x) { dismo::evaluate(pres_test, backg_test, model = x$model)@auc })), AIC = unlist(lapply(modellist, function(x) { AIC(x$model) }))) df$delAIC &lt;- df$AIC - min(df$AIC) df &lt;- df[order(df$AIC), ] knitr::kable(df, row.names = FALSE) name Spearman AUC AIC delAIC gamVIF4 0.939 0.7897686 2512.852 0.00000 gamVIF2 0.930 0.7701322 2552.164 39.31228 gamEnv4 0.904 0.7664959 2579.006 66.15366 gamEnv2 0.939 0.7521983 2633.808 120.95618 glmStep 0.939 0.7425372 2648.513 135.66075 gamLC4 0.777 0.7331653 2650.557 137.70542 glmVIF 0.948 0.7430992 2656.807 143.95475 gamLC2 0.873 0.7223140 2688.271 175.41892 gamEnvMin 0.832 0.7215950 2711.681 198.82886 glmLC 0.902 0.7117438 2720.935 208.08274 gamTopo4 0.832 0.7281488 2739.883 227.03084 glmEnv 0.811 0.7183471 2749.253 236.40140 gamTopo2 0.873 0.7203802 2758.860 246.00769 glmTopo 0.856 0.7145455 2775.568 262.71597 glmMinEnv 0.865 0.6961157 2780.547 267.69485 9.10 Save Save. save(gamfm, pm.plot, modellist, file = &quot;modellist.RData&quot;) "]
]
